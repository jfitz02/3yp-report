{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid.\n",
      "Your token has been saved to C:\\Users\\jfitz\\.huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "#get token from pass.secret\n",
    "with open(\"pass.secret\", \"r\") as f:\n",
    "    token = f.read()\n",
    "    token = token.strip()\n",
    "\n",
    "#login to huggingface\n",
    "login(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               label                                               text\n",
      "32194  entertainment              michelle yeoh laughs yes finally cool\n",
      "32195       medicine  slovenia in slovenia medical graduates after s...\n",
      "32196     philosophy  spanish philosopher g√≥mez pereira in his 1554 ...\n",
      "32197          music  billie jean won two grammy awards best rb song...\n",
      "32198           news      utilities triple battery storage capacity eia\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mathematics</td>\n",
       "      <td>parametricaly describe roughness printed cupol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>technology</td>\n",
       "      <td>citeseerx 10116599379 doi101177146499341001100303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>geography</td>\n",
       "      <td>accurately determine new coordinates dms point...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sports</td>\n",
       "      <td>rocker boards offer only one degree of movemen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>in japan during the edo period flatulists were...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           label                                               text\n",
       "0    mathematics  parametricaly describe roughness printed cupol...\n",
       "1     technology  citeseerx 10116599379 doi101177146499341001100303\n",
       "2      geography  accurately determine new coordinates dms point...\n",
       "3         sports  rocker boards offer only one degree of movemen...\n",
       "4  entertainment  in japan during the edo period flatulists were..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data and add column names\n",
    "data = pd.read_csv(\"../testdata/data_combined_24.csv\", delimiter=\",\", header=None, names=[\"label\", \"text\"])\n",
    "\n",
    "#randomly shuffle the data\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#split into train and test (90/10)\n",
    "train_data = data[:int(len(data)*0.9)]\n",
    "test_data = data[int(len(data)*0.9)+1:]\n",
    "print(test_data.head())\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m RobertaForSequenceClassification\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mroberta-base\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m tokenizer \u001b[39m=\u001b[39m RobertaTokenizerFast\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mroberta-base\u001b[39m\u001b[39m'\u001b[39m, max_length \u001b[39m=\u001b[39m \u001b[39m512\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\transformers\\modeling_utils.py:2012\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[0;32m   2011\u001b[0m     config_path \u001b[39m=\u001b[39m config \u001b[39mif\u001b[39;00m config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m pretrained_model_name_or_path\n\u001b[1;32m-> 2012\u001b[0m     config, model_kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mconfig_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m   2013\u001b[0m         config_path,\n\u001b[0;32m   2014\u001b[0m         cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[0;32m   2015\u001b[0m         return_unused_kwargs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   2016\u001b[0m         force_download\u001b[39m=\u001b[39mforce_download,\n\u001b[0;32m   2017\u001b[0m         resume_download\u001b[39m=\u001b[39mresume_download,\n\u001b[0;32m   2018\u001b[0m         proxies\u001b[39m=\u001b[39mproxies,\n\u001b[0;32m   2019\u001b[0m         local_files_only\u001b[39m=\u001b[39mlocal_files_only,\n\u001b[0;32m   2020\u001b[0m         use_auth_token\u001b[39m=\u001b[39muse_auth_token,\n\u001b[0;32m   2021\u001b[0m         revision\u001b[39m=\u001b[39mrevision,\n\u001b[0;32m   2022\u001b[0m         subfolder\u001b[39m=\u001b[39msubfolder,\n\u001b[0;32m   2023\u001b[0m         _from_auto\u001b[39m=\u001b[39mfrom_auto_class,\n\u001b[0;32m   2024\u001b[0m         _from_pipeline\u001b[39m=\u001b[39mfrom_pipeline,\n\u001b[0;32m   2025\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2026\u001b[0m     )\n\u001b[0;32m   2027\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2028\u001b[0m     model_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\transformers\\configuration_utils.py:532\u001b[0m, in \u001b[0;36mPretrainedConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    455\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\u001b[39mcls\u001b[39m, pretrained_model_name_or_path: Union[\u001b[39mstr\u001b[39m, os\u001b[39m.\u001b[39mPathLike], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPretrainedConfig\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[39m    Instantiate a [`PretrainedConfig`] (or a derived class) from a pretrained model configuration.\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[39m    assert unused_kwargs == {\"foo\": False}\u001b[39;00m\n\u001b[0;32m    531\u001b[0m \u001b[39m    ```\"\"\"\u001b[39;00m\n\u001b[1;32m--> 532\u001b[0m     config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    533\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodel_type:\n\u001b[0;32m    534\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    535\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou are using a model of type \u001b[39m\u001b[39m{\u001b[39;00mconfig_dict[\u001b[39m'\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m to instantiate a model of type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    536\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodel_type\u001b[39m}\u001b[39;00m\u001b[39m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    537\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\transformers\\configuration_utils.py:559\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    557\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[0;32m    558\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[1;32m--> 559\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    560\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[0;32m    561\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\transformers\\configuration_utils.py:614\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    610\u001b[0m configuration_file \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_configuration_file\u001b[39m\u001b[39m\"\u001b[39m, CONFIG_NAME)\n\u001b[0;32m    612\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    613\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[1;32m--> 614\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[0;32m    615\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m    616\u001b[0m         configuration_file,\n\u001b[0;32m    617\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    618\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    619\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    620\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    621\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    622\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    623\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    624\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    625\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[0;32m    626\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[0;32m    627\u001b[0m     )\n\u001b[0;32m    628\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m    629\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[0;32m    630\u001b[0m     \u001b[39m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[0;32m    631\u001b[0m     \u001b[39m# the original exception.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\transformers\\utils\\hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    406\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    407\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    408\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 409\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    410\u001b[0m         path_or_repo_id,\n\u001b[0;32m    411\u001b[0m         filename,\n\u001b[0;32m    412\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    413\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    414\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    415\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    416\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    417\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    418\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    419\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    420\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    421\u001b[0m     )\n\u001b[0;32m    423\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[0;32m    424\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    425\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    426\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    427\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    428\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    429\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:124\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    120\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(\n\u001b[0;32m    121\u001b[0m         fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs\n\u001b[0;32m    122\u001b[0m     )\n\u001b[1;32m--> 124\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1067\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1065\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1066\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1067\u001b[0m         metadata \u001b[39m=\u001b[39m get_hf_file_metadata(\n\u001b[0;32m   1068\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m   1069\u001b[0m             token\u001b[39m=\u001b[39;49mtoken,\n\u001b[0;32m   1070\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   1071\u001b[0m             timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[0;32m   1072\u001b[0m         )\n\u001b[0;32m   1073\u001b[0m     \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n\u001b[0;32m   1074\u001b[0m         \u001b[39m# Cache the non-existence of the file and raise\u001b[39;00m\n\u001b[0;32m   1075\u001b[0m         commit_hash \u001b[39m=\u001b[39m http_error\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\n\u001b[0;32m   1076\u001b[0m             HUGGINGFACE_HEADER_X_REPO_COMMIT\n\u001b[0;32m   1077\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:124\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    120\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(\n\u001b[0;32m    121\u001b[0m         fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs\n\u001b[0;32m    122\u001b[0m     )\n\u001b[1;32m--> 124\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1367\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[0;32m   1364\u001b[0m headers \u001b[39m=\u001b[39m build_hf_headers(token\u001b[39m=\u001b[39mtoken)\n\u001b[0;32m   1366\u001b[0m \u001b[39m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m-> 1367\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[0;32m   1368\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1369\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m   1370\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m   1371\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1372\u001b[0m     follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1373\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   1374\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m   1375\u001b[0m )\n\u001b[0;32m   1376\u001b[0m hf_raise_for_status(r)\n\u001b[0;32m   1378\u001b[0m \u001b[39m# Return\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:403\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[39m# 2. Force relative redirection\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[39mif\u001b[39;00m follow_relative_redirects:\n\u001b[1;32m--> 403\u001b[0m     response \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[0;32m    404\u001b[0m         method\u001b[39m=\u001b[39mmethod,\n\u001b[0;32m    405\u001b[0m         url\u001b[39m=\u001b[39murl,\n\u001b[0;32m    406\u001b[0m         max_retries\u001b[39m=\u001b[39mmax_retries,\n\u001b[0;32m    407\u001b[0m         base_wait_time\u001b[39m=\u001b[39mbase_wait_time,\n\u001b[0;32m    408\u001b[0m         max_wait_time\u001b[39m=\u001b[39mmax_wait_time,\n\u001b[0;32m    409\u001b[0m         timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[0;32m    410\u001b[0m         follow_relative_redirects\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    411\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    412\u001b[0m     )\n\u001b[0;32m    414\u001b[0m     \u001b[39m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[0;32m    415\u001b[0m     \u001b[39m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[0;32m    416\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m300\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mstatus_code \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m399\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:438\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    435\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n\u001b[0;32m    437\u001b[0m \u001b[39m# 3. Exponential backoff\u001b[39;00m\n\u001b[1;32m--> 438\u001b[0m \u001b[39mreturn\u001b[39;00m http_backoff(\n\u001b[0;32m    439\u001b[0m     method\u001b[39m=\u001b[39mmethod,\n\u001b[0;32m    440\u001b[0m     url\u001b[39m=\u001b[39murl,\n\u001b[0;32m    441\u001b[0m     max_retries\u001b[39m=\u001b[39mmax_retries,\n\u001b[0;32m    442\u001b[0m     base_wait_time\u001b[39m=\u001b[39mbase_wait_time,\n\u001b[0;32m    443\u001b[0m     max_wait_time\u001b[39m=\u001b[39mmax_wait_time,\n\u001b[0;32m    444\u001b[0m     retry_on_exceptions\u001b[39m=\u001b[39m(ConnectTimeout, ProxyError),\n\u001b[0;32m    445\u001b[0m     retry_on_status_codes\u001b[39m=\u001b[39m(),\n\u001b[0;32m    446\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[0;32m    447\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    448\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:129\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[1;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mseek(io_obj_initial_pos)\n\u001b[0;32m    128\u001b[0m \u001b[39m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    130\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m retry_on_status_codes:\n\u001b[0;32m    131\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\requests\\sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    585\u001b[0m }\n\u001b[0;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\requests\\sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\requests\\adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[1;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    500\u001b[0m         )\n\u001b[0;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    704\u001b[0m     conn,\n\u001b[0;32m    705\u001b[0m     method,\n\u001b[0;32m    706\u001b[0m     url,\n\u001b[0;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    711\u001b[0m )\n\u001b[0;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[0;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[39m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 386\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[0;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    388\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[0;32m    389\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mconn\u001b[39m.\u001b[39mtimeout)\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[39m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> 1042\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n\u001b[0;32m   1045\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1046\u001b[0m         (\n\u001b[0;32m   1047\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnverified HTTPS request is being made to host \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1052\u001b[0m         InsecureRequestWarning,\n\u001b[0;32m   1053\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\urllib3\\connection.py:358\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    357\u001b[0m     \u001b[39m# Add certificate verification\u001b[39;00m\n\u001b[1;32m--> 358\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[0;32m    359\u001b[0m     hostname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n\u001b[0;32m    360\u001b[0m     tls_in_tls \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\urllib3\\connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    171\u001b[0m     extra_kw[\u001b[39m\"\u001b[39m\u001b[39msocket_options\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket_options\n\u001b[0;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     conn \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39mcreate_connection(\n\u001b[0;32m    175\u001b[0m         (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dns_host, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mport), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mextra_kw\n\u001b[0;32m    176\u001b[0m     )\n\u001b[0;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n\u001b[0;32m    179\u001b[0m     \u001b[39mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[0;32m    180\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m    181\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConnection to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m timed out. (connect timeout=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    182\u001b[0m         \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout),\n\u001b[0;32m    183\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[39mif\u001b[39;00m source_address:\n\u001b[0;32m     84\u001b[0m         sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 85\u001b[0m     sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[0;32m     86\u001b[0m     \u001b[39mreturn\u001b[39;00m sock\n\u001b[0;32m     88\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39merror \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', max_length = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "# tokenize the data adding padding so that all sequences are the same length\n",
    "t_train_data = tokenizer(train_data.text.tolist(), truncation=True, padding=True)\n",
    "t_test_data = tokenizer(test_data.text.tolist(), truncation=True, padding=True)\n",
    "\n",
    "print(t_train_data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 35901, 21612, 461, 9857, 27711, 3763, 13617, 2489, 337, 463, 5378, 1043, 1071, 225, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# get all input_ids\n",
    "train_input_ids = []\n",
    "train_attention_masks = []\n",
    "\n",
    "for i in range(len(t_train_data[\"input_ids\"])):\n",
    "    train_input_ids.append(t_train_data['input_ids'][i])\n",
    "    train_attention_masks.append(t_train_data['attention_mask'][i])\n",
    "\n",
    "test_input_ids = []\n",
    "test_attention_masks = []\n",
    "\n",
    "for i in range(len(t_test_data[\"input_ids\"])):\n",
    "    test_input_ids.append(t_test_data['input_ids'][i])\n",
    "    test_attention_masks.append(t_test_data['attention_mask'][i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all unique topics\n",
    "topics = data['label'].unique()\n",
    "\n",
    "# create mapping from topic to integer\n",
    "topic_to_int = {}\n",
    "for i in range(len(topics)):\n",
    "    topic_to_int[topics[i]] = i\n",
    "\n",
    "# create mapping from integer to topic\n",
    "int_to_topic = {}\n",
    "for i in range(len(topics)):\n",
    "    int_to_topic[i] = topics[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jfitz\\AppData\\Local\\Temp\\ipykernel_17604\\1302675792.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_data_np = np.array([train_input_ids, train_attention_masks, train_data['label']]).T\n",
      "C:\\Users\\jfitz\\AppData\\Local\\Temp\\ipykernel_17604\\1302675792.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  test_data_np = np.array([test_input_ids, test_attention_masks, test_data['label']]).T\n"
     ]
    }
   ],
   "source": [
    "# train_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "# test_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# create np array combining input_ids and attention_mask and label\n",
    "train_data_np = np.array([train_input_ids, train_attention_masks, train_data['label']]).T\n",
    "test_data_np = np.array([test_input_ids, test_attention_masks, test_data['label']]).T\n",
    "\n",
    "final_train = pd.DataFrame(train_data_np, columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "final_test = pd.DataFrame(test_data_np, columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# map label to integer\n",
    "if type(final_train['label'][0]) == str:\n",
    "    final_train['label'] = final_train['label'].map(lambda x: topic_to_int[x])\n",
    "\n",
    "if type(final_test['label'][0]) == str:\n",
    "    final_test['label'] = final_test['label'].map(lambda x: topic_to_int[x])\n",
    "\n",
    "\n",
    "dataframe_train = datasets.Dataset.from_pandas(final_train)\n",
    "dataframe_test = datasets.Dataset.from_pandas(final_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    f\"roberta-finetuned-topic\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\BERT\\RoBERTa\\roberta-finetuned-topic is already a clone of https://huggingface.co/MrFitzmaurice/roberta-finetuned-topic. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=dataframe_train,\n",
    "    eval_dataset=dataframe_test,\n",
    "    # tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 32193\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 20125\n",
      "  Number of trainable parameters = 124647170\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d318005711a4ebdb0fb1cd689f1376d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\transformers\\trainer.py:1527\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1524\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1525\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1526\u001b[0m )\n\u001b[1;32m-> 1527\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1528\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1529\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1530\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1531\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1532\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\transformers\\trainer.py:1775\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1773\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1774\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1777\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1778\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1779\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1780\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1781\u001b[0m ):\n\u001b[0;32m   1782\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\transformers\\trainer.py:2523\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2520\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m   2522\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2523\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[0;32m   2525\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   2526\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\transformers\\trainer.py:2555\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2553\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2554\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2555\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[0;32m   2556\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2557\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2558\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1215\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1207\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1208\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1209\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1210\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1211\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1212\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1213\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1215\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[0;32m   1216\u001b[0m     input_ids,\n\u001b[0;32m   1217\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1218\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1219\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1220\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1221\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1222\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1223\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1224\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1225\u001b[0m )\n\u001b[0;32m   1226\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1227\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:853\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    844\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    846\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m    847\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m    848\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    851\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    852\u001b[0m )\n\u001b[1;32m--> 853\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m    854\u001b[0m     embedding_output,\n\u001b[0;32m    855\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m    856\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    857\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    858\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m    859\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    860\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    861\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    862\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    863\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    864\u001b[0m )\n\u001b[0;32m    865\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    866\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:527\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    518\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    519\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    520\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    524\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    525\u001b[0m     )\n\u001b[0;32m    526\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 527\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    528\u001b[0m         hidden_states,\n\u001b[0;32m    529\u001b[0m         attention_mask,\n\u001b[0;32m    530\u001b[0m         layer_head_mask,\n\u001b[0;32m    531\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    532\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    533\u001b[0m         past_key_value,\n\u001b[0;32m    534\u001b[0m         output_attentions,\n\u001b[0;32m    535\u001b[0m     )\n\u001b[0;32m    537\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    538\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:412\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    401\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    402\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    409\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    410\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    411\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 412\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    413\u001b[0m         hidden_states,\n\u001b[0;32m    414\u001b[0m         attention_mask,\n\u001b[0;32m    415\u001b[0m         head_mask,\n\u001b[0;32m    416\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    417\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    418\u001b[0m     )\n\u001b[0;32m    419\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    421\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:339\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    330\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    331\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    337\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    338\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 339\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    340\u001b[0m         hidden_states,\n\u001b[0;32m    341\u001b[0m         attention_mask,\n\u001b[0;32m    342\u001b[0m         head_mask,\n\u001b[0;32m    343\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    344\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    345\u001b[0m         past_key_value,\n\u001b[0;32m    346\u001b[0m         output_attentions,\n\u001b[0;32m    347\u001b[0m     )\n\u001b[0;32m    348\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    349\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:235\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    232\u001b[0m     past_key_value \u001b[39m=\u001b[39m (key_layer, value_layer)\n\u001b[0;32m    234\u001b[0m \u001b[39m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[1;32m--> 235\u001b[0m attention_scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(query_layer, key_layer\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m))\n\u001b[0;32m    237\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrelative_key\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrelative_key_query\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    238\u001b[0m     query_length, key_length \u001b[39m=\u001b[39m query_layer\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m], key_layer\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97999f904c5bcf44446ad9fa0f3977d04350a64ad2233c4cf3d5a942bd2f4055"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
