{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification,Trainer, TrainingArguments\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (C:/Users/jfitz/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n",
      "100%|██████████| 2/2 [00:00<00:00, 19.80it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = datasets.load_dataset('imdb', split =['train', 'test'])\n",
    "\n",
    "# limit the dataset to 1000 samples\n",
    "train_data = train_data.select(range(1000))\n",
    "test_data = test_data.select(range(1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', max_length = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.54ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.25ba/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenization(batched_text):\n",
    "    return tokenizer(batched_text['text'], padding = True, truncation=True)\n",
    "\n",
    "\n",
    "train_data = train_data.map(tokenization, batched = True, batch_size = len(train_data))\n",
    "test_data = test_data.map(tokenization, batched = True, batch_size = len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = './results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size = 4,\n",
    "    gradient_accumulation_steps = 16,    \n",
    "    per_device_eval_batch_size= 8,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    disable_tqdm = False, \n",
    "    load_best_model_at_end=True,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps = 8,\n",
    "    logging_dir='./logs',\n",
    "    dataloader_num_workers = 8,\n",
    "    run_name = 'roberta-classification'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "c:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 45\n",
      "  Number of trainable parameters = 124647170\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m API key must be 40 characters long, yours was 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\wandb\\sdk\\wandb_init.py\", line 1075, in init\n",
      "    wi.setup(kwargs)\n",
      "  File \"c:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\wandb\\sdk\\wandb_init.py\", line 274, in setup\n",
      "    wandb_login._login(\n",
      "  File \"c:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\wandb\\sdk\\wandb_login.py\", line 298, in _login\n",
      "    wlogin.prompt_api_key()\n",
      "  File \"c:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\wandb\\sdk\\wandb_login.py\", line 221, in prompt_api_key\n",
      "    key, status = self._prompt_api_key()\n",
      "  File \"c:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\wandb\\sdk\\wandb_login.py\", line 201, in _prompt_api_key\n",
      "    key = apikey.prompt_api_key(\n",
      "  File \"c:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\wandb\\sdk\\lib\\apikey.py\", line 134, in prompt_api_key\n",
      "    key = input_callback(api_ask).strip()\n",
      "  File \"c:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\click\\termui.py\", line 166, in prompt\n",
      "    value = prompt_func(prompt)\n",
      "  File \"c:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\click\\termui.py\", line 149, in prompt_func\n",
      "    raise Abort() from None\n",
      "click.exceptions.Abort\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Abnormal program exit\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "problem",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAbort\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\wandb\\sdk\\wandb_init.py:1075\u001b[0m, in \u001b[0;36minit\u001b[1;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[0;32m   1074\u001b[0m wi \u001b[39m=\u001b[39m _WandbInit()\n\u001b[1;32m-> 1075\u001b[0m wi\u001b[39m.\u001b[39;49msetup(kwargs)\n\u001b[0;32m   1076\u001b[0m except_exit \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39m_except_exit\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\wandb\\sdk\\wandb_init.py:274\u001b[0m, in \u001b[0;36m_WandbInit.setup\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m settings\u001b[39m.\u001b[39m_offline \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m settings\u001b[39m.\u001b[39m_noop:\n\u001b[1;32m--> 274\u001b[0m     wandb_login\u001b[39m.\u001b[39;49m_login(\n\u001b[0;32m    275\u001b[0m         anonymous\u001b[39m=\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39manonymous\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    276\u001b[0m         force\u001b[39m=\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mforce\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    277\u001b[0m         _disable_warning\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    278\u001b[0m         _silent\u001b[39m=\u001b[39;49msettings\u001b[39m.\u001b[39;49mquiet \u001b[39mor\u001b[39;49;00m settings\u001b[39m.\u001b[39;49msilent,\n\u001b[0;32m    279\u001b[0m         _entity\u001b[39m=\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mentity\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mor\u001b[39;49;00m settings\u001b[39m.\u001b[39;49mentity,\n\u001b[0;32m    280\u001b[0m     )\n\u001b[0;32m    282\u001b[0m \u001b[39m# apply updated global state after login was handled\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\wandb\\sdk\\wandb_login.py:298\u001b[0m, in \u001b[0;36m_login\u001b[1;34m(anonymous, key, relogin, host, force, timeout, _backend, _silent, _disable_warning, _entity)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m key:\n\u001b[1;32m--> 298\u001b[0m     wlogin\u001b[39m.\u001b[39;49mprompt_api_key()\n\u001b[0;32m    300\u001b[0m \u001b[39m# make sure login credentials get to the backend\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\wandb\\sdk\\wandb_login.py:221\u001b[0m, in \u001b[0;36m_WandbLogin.prompt_api_key\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprompt_api_key\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 221\u001b[0m     key, status \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prompt_api_key()\n\u001b[0;32m    222\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m==\u001b[39m ApiKeyStatus\u001b[39m.\u001b[39mNOTTY:\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\wandb\\sdk\\wandb_login.py:201\u001b[0m, in \u001b[0;36m_WandbLogin._prompt_api_key\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m     key \u001b[39m=\u001b[39m apikey\u001b[39m.\u001b[39;49mprompt_api_key(\n\u001b[0;32m    202\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_settings,\n\u001b[0;32m    203\u001b[0m         api\u001b[39m=\u001b[39;49mapi,\n\u001b[0;32m    204\u001b[0m         no_offline\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_settings\u001b[39m.\u001b[39;49mforce \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_settings \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    205\u001b[0m         no_create\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_settings\u001b[39m.\u001b[39;49mforce \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_settings \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    206\u001b[0m     )\n\u001b[0;32m    207\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    208\u001b[0m     \u001b[39m# invalid key provided, try again\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\wandb\\sdk\\lib\\apikey.py:134\u001b[0m, in \u001b[0;36mprompt_api_key\u001b[1;34m(settings, api, input_callback, browser_callback, no_offline, no_create, local)\u001b[0m\n\u001b[0;32m    131\u001b[0m     wandb\u001b[39m.\u001b[39mtermlog(\n\u001b[0;32m    132\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou can find your API key in your browser here: \u001b[39m\u001b[39m{\u001b[39;00mapp_url\u001b[39m}\u001b[39;00m\u001b[39m/authorize\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    133\u001b[0m     )\n\u001b[1;32m--> 134\u001b[0m     key \u001b[39m=\u001b[39m input_callback(api_ask)\u001b[39m.\u001b[39mstrip()\n\u001b[0;32m    135\u001b[0m write_key(settings, key, api\u001b[39m=\u001b[39mapi)\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\click\\termui.py:166\u001b[0m, in \u001b[0;36mprompt\u001b[1;34m(text, default, hide_input, confirmation_prompt, type, value_proc, prompt_suffix, show_default, err, show_choices)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 166\u001b[0m     value \u001b[39m=\u001b[39m prompt_func(prompt)\n\u001b[0;32m    167\u001b[0m     \u001b[39mif\u001b[39;00m value:\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\click\\termui.py:149\u001b[0m, in \u001b[0;36mprompt.<locals>.prompt_func\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    148\u001b[0m     echo(\u001b[39mNone\u001b[39;00m, err\u001b[39m=\u001b[39merr)\n\u001b[1;32m--> 149\u001b[0m \u001b[39mraise\u001b[39;00m Abort() \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;31mAbort\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\transformers\\trainer.py:1527\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1524\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1525\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1526\u001b[0m )\n\u001b[1;32m-> 1527\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1528\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1529\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1530\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1531\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1532\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\transformers\\trainer.py:1704\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1701\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step\n\u001b[0;32m   1702\u001b[0m model\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m-> 1704\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcallback_handler\u001b[39m.\u001b[39;49mon_train_begin(args, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontrol)\n\u001b[0;32m   1706\u001b[0m \u001b[39m# Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.\u001b[39;00m\n\u001b[0;32m   1707\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m args\u001b[39m.\u001b[39mignore_data_skip:\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\transformers\\trainer_callback.py:353\u001b[0m, in \u001b[0;36mCallbackHandler.on_train_begin\u001b[1;34m(self, args, state, control)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_begin\u001b[39m(\u001b[39mself\u001b[39m, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n\u001b[0;32m    352\u001b[0m     control\u001b[39m.\u001b[39mshould_training_stop \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_event(\u001b[39m\"\u001b[39;49m\u001b[39mon_train_begin\u001b[39;49m\u001b[39m\"\u001b[39;49m, args, state, control)\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\transformers\\trainer_callback.py:397\u001b[0m, in \u001b[0;36mCallbackHandler.call_event\u001b[1;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_event\u001b[39m(\u001b[39mself\u001b[39m, event, args, state, control, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    396\u001b[0m     \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m--> 397\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, event)(\n\u001b[0;32m    398\u001b[0m             args,\n\u001b[0;32m    399\u001b[0m             state,\n\u001b[0;32m    400\u001b[0m             control,\n\u001b[0;32m    401\u001b[0m             model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel,\n\u001b[0;32m    402\u001b[0m             tokenizer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer,\n\u001b[0;32m    403\u001b[0m             optimizer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer,\n\u001b[0;32m    404\u001b[0m             lr_scheduler\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_scheduler,\n\u001b[0;32m    405\u001b[0m             train_dataloader\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dataloader,\n\u001b[0;32m    406\u001b[0m             eval_dataloader\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_dataloader,\n\u001b[0;32m    407\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    408\u001b[0m         )\n\u001b[0;32m    409\u001b[0m         \u001b[39m# A Callback can skip the return of `control` if it doesn't change it.\u001b[39;00m\n\u001b[0;32m    410\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\transformers\\integrations.py:731\u001b[0m, in \u001b[0;36mWandbCallback.on_train_begin\u001b[1;34m(self, args, state, control, model, **kwargs)\u001b[0m\n\u001b[0;32m    729\u001b[0m     args\u001b[39m.\u001b[39mrun_name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    730\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initialized:\n\u001b[1;32m--> 731\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msetup(args, state, model, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\transformers\\integrations.py:703\u001b[0m, in \u001b[0;36mWandbCallback.setup\u001b[1;34m(self, args, state, model, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m     run_name \u001b[39m=\u001b[39m args\u001b[39m.\u001b[39mrun_name\n\u001b[0;32m    702\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wandb\u001b[39m.\u001b[39mrun \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 703\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wandb\u001b[39m.\u001b[39minit(\n\u001b[0;32m    704\u001b[0m         project\u001b[39m=\u001b[39mos\u001b[39m.\u001b[39mgetenv(\u001b[39m\"\u001b[39m\u001b[39mWANDB_PROJECT\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mhuggingface\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    705\u001b[0m         name\u001b[39m=\u001b[39mrun_name,\n\u001b[0;32m    706\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minit_args,\n\u001b[0;32m    707\u001b[0m     )\n\u001b[0;32m    708\u001b[0m \u001b[39m# add config parameters (run may have been created manually)\u001b[39;00m\n\u001b[0;32m    709\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wandb\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mupdate(combined_dict, allow_val_change\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\jfitz\\OneDrive\\Documents\\3rd year project\\code\\venv\\lib\\site-packages\\wandb\\sdk\\wandb_init.py:1115\u001b[0m, in \u001b[0;36minit\u001b[1;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[39mif\u001b[39;00m except_exit:\n\u001b[0;32m   1114\u001b[0m             os\u001b[39m.\u001b[39m_exit(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m-> 1115\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mproblem\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39merror_seen\u001b[39;00m\n\u001b[0;32m   1116\u001b[0m \u001b[39mreturn\u001b[39;00m run\n",
      "\u001b[1;31mException\u001b[0m: problem"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97999f904c5bcf44446ad9fa0f3977d04350a64ad2233c4cf3d5a942bd2f4055"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
